# 日志系统

### ELK体系

##### ELK是什么

ELK是ElasticSearch、Logstash、Kibana三个应用的缩写。 ElasticSearch简称ES，主要用来存储和检索数据。Logstash主要用来往ES中写入数据。Kibana主要用来展示数据。


##### 为什么要用ELK

传统的社工库通常用MySQL数据库来进行搭建，在相当大的数据下检索效率非常低下。在这种关系型数据库中进行查询需要明确指定列名。而在ES中可用全文检索，并且在大数据的查询中的响应几乎都是毫秒级的，速度相当之快！ELK原本用在日志的大数据收集和分析，其可怕的速度作为社工库也是一种不错的选择。


### heka

heka 是 Mozilla 公司仿造 logstash 设计，用 Golang 重写的一个开源项目。同样采用了input -> decoder -> filter -> encoder -> output 的流程概念。其特点在于，在中间的 decoder/filter/encoder 部分，设计了 sandbox 概念，可以采用内嵌 lua 脚本做这一部分的工作，降低了全程使用静态 Golang 编写的难度。此外，其 filter 阶段还提供了一些监控和统计报警功能。

[heka官网](http://hekad.readthedocs.org/)

[heka学习](http://kibana.logstash.es/content/logstash/scale/heka.html)


heka 目前仿造的还是旧版本的 logstash schema 设计，所有切分字段都存储在 @fields 下。
经测试，其处理性能跟开启了多线程 filters 的 logstash 进程类似，都在每秒 30000 条。


##### heka 插件

`inputs`

来源： 文件、socket、消息队列(MQ、KAFKA)

处理： 必须用go。

`splitters`

被inputs使用，切分为单独的记录

处理： 必须用go


`decoders`

来源： inputs

处理： go或者lua黑盒。将其转换为Heka Message structs

`filters`

来源：

处理：go或者lua黑盒。heka处理引擎，根据 Message Matcher Syntax作消息过滤和分发。捕获消息，作监控、聚合和处理。 产生新消息（eg: summary message）插入到heka pipeline。支持配置lua动态插入到lua instance，不需要重启heka。

[匹配规则](http://hekad.readthedocs.io/en/v0.10.0/message_matcher.html#message-matcher)


`encoders`

来源： heka pipeline。

处理： go或者lua黑盒。和decoders相反，将Heka Message structs转换为外部输出流。被嵌入在outputs插件中使用。
 

`outpus`

来源： heka pipeline。 

处理： 必须用go。利用encoders做数据处理，将序列化后的数据发送给外部。利用 Message Matcher Syntax进行过滤和分发。

##### hekad

定位： heka核心hekad，A single hekad process can be configured with any number of plugins, 
simultaneously performing a variety of data gathering, processing, and shipping tasks. 

[hekad配置](http://hekad.readthedocs.io/en/v0.10.0/config/index.html#configuration)


##### heka 监控

[heka监控](http://hekad.readthedocs.io/en/v0.10.0/monitoring/index.html)

##### heka 测试

[heka测试](http://hekad.readthedocs.io/en/v0.10.0/developing/testing.html)


##### 













